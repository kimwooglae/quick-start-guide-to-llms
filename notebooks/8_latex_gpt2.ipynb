{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e044664f",
   "metadata": {},
   "source": [
    "## GPT2를 이용한 LaTex 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4138f-7c63-4813-b2d2-501207dabb16",
   "metadata": {},
   "source": [
    "# 1. 모델 로딩 및 데이터 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65cd4e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TextDataset, DataCollatorForLanguageModeling, AutoModelForCausalLM, pipeline, \\\n",
    "                         Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b74db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL = 'gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)  # load up a standard gpt2 model\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # set the pad token to avoid a warning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4a8564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>LaTeX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>integral from a to b of x squared</td>\n",
       "      <td>\\int_{a}^{b} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>integral from negative 1 to 1 of x squared</td>\n",
       "      <td>\\int_{-1}^{1} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      English                   LaTeX\n",
       "0           integral from a to b of x squared   \\int_{a}^{b} x^2 \\,dx\n",
       "1  integral from negative 1 to 1 of x squared  \\int_{-1}^{1} x^2 \\,dx"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/english_to_latex.csv')\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a166ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>LaTeX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>integral from a to b of x squared</td>\n",
       "      <td>\\int_{a}^{b} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>integral from negative 1 to 1 of x squared</td>\n",
       "      <td>\\int_{-1}^{1} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>integral from negative 1 to infinity of x cubed</td>\n",
       "      <td>\\int_{-1}^{\\inf} x^3 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>integral from 0 to infinity of x squared</td>\n",
       "      <td>\\int_{0}^{\\inf} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>integral from 0 to infinity of y squared</td>\n",
       "      <td>\\int_{0}^{\\inf} y^2 \\,dy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>integral from 1 to 2 of x over 2</td>\n",
       "      <td>\\int_{1}^{2} \\frac{x}{2} \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>f of x equals x squared</td>\n",
       "      <td>f(x) = x^2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>h of x equals x squared</td>\n",
       "      <td>h(x) = x^2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>g of x equals x squared</td>\n",
       "      <td>g(x) = x^2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>g of x equals x to the eighth power</td>\n",
       "      <td>g(x) = x^8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           English  \\\n",
       "0                integral from a to b of x squared   \n",
       "1       integral from negative 1 to 1 of x squared   \n",
       "2  integral from negative 1 to infinity of x cubed   \n",
       "3         integral from 0 to infinity of x squared   \n",
       "4         integral from 0 to infinity of y squared   \n",
       "5                 integral from 1 to 2 of x over 2   \n",
       "6                          f of x equals x squared   \n",
       "7                          h of x equals x squared   \n",
       "8                          g of x equals x squared   \n",
       "9              g of x equals x to the eighth power   \n",
       "\n",
       "                           LaTeX  \n",
       "0          \\int_{a}^{b} x^2 \\,dx  \n",
       "1         \\int_{-1}^{1} x^2 \\,dx  \n",
       "2      \\int_{-1}^{\\inf} x^3 \\,dx  \n",
       "3       \\int_{0}^{\\inf} x^2 \\,dx  \n",
       "4       \\int_{0}^{\\inf} y^2 \\,dy  \n",
       "5  \\int_{1}^{2} \\frac{x}{2} \\,dx  \n",
       "6                     f(x) = x^2  \n",
       "7                     h(x) = x^2  \n",
       "8                     g(x) = x^2  \n",
       "9                     g(x) = x^8  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdbfb33-615c-4f90-8250-2f621f299f41",
   "metadata": {},
   "source": [
    "# 2. 프롬프트 생성 및 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db5ebf1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Convert English to LaTeX\\nEnglish: integral fr...\n",
       "1     Convert English to LaTeX\\nEnglish: integral fr...\n",
       "2     Convert English to LaTeX\\nEnglish: integral fr...\n",
       "3     Convert English to LaTeX\\nEnglish: integral fr...\n",
       "4     Convert English to LaTeX\\nEnglish: integral fr...\n",
       "5     Convert English to LaTeX\\nEnglish: integral fr...\n",
       "6     Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "7     Convert English to LaTeX\\nEnglish: h of x equa...\n",
       "8     Convert English to LaTeX\\nEnglish: g of x equa...\n",
       "9     Convert English to LaTeX\\nEnglish: g of x equa...\n",
       "10    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "11    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "12    Convert English to LaTeX\\nEnglish: h of x equa...\n",
       "13    Convert English to LaTeX\\nEnglish: g of x equa...\n",
       "14    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "15    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "16    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "17    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "18    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "19    Convert English to LaTeX\\nEnglish: x squared\\n...\n",
       "20    Convert English to LaTeX\\nEnglish: x cubed\\nLa...\n",
       "21    Convert English to LaTeX\\nEnglish: pi squared\\...\n",
       "22    Convert English to LaTeX\\nEnglish: z squared\\n...\n",
       "23    Convert English to LaTeX\\nEnglish: z over x sq...\n",
       "24    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "25    Convert English to LaTeX\\nEnglish: 1 over 6\\nL...\n",
       "26    Convert English to LaTeX\\nEnglish: 2 pi\\nLaTeX...\n",
       "27    Convert English to LaTeX\\nEnglish: s cubed\\nLa...\n",
       "28    Convert English to LaTeX\\nEnglish: s to the si...\n",
       "29    Convert English to LaTeX\\nEnglish: 2 pi r\\nLaT...\n",
       "30    Convert English to LaTeX\\nEnglish: pi over n\\n...\n",
       "31    Convert English to LaTeX\\nEnglish: f of n equa...\n",
       "32    Convert English to LaTeX\\nEnglish: pi times x\\...\n",
       "33    Convert English to LaTeX\\nEnglish: pi to the f...\n",
       "34    Convert English to LaTeX\\nEnglish: pi to the f...\n",
       "35    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "36    Convert English to LaTeX\\nEnglish: g of x equa...\n",
       "37    Convert English to LaTeX\\nEnglish: g of x equa...\n",
       "38    Convert English to LaTeX\\nEnglish: 1 over n\\nL...\n",
       "39    Convert English to LaTeX\\nEnglish: x squared o...\n",
       "40    Convert English to LaTeX\\nEnglish: y squared o...\n",
       "41    Convert English to LaTeX\\nEnglish: 1 over 7 to...\n",
       "42    Convert English to LaTeX\\nEnglish: 1 over 9 to...\n",
       "43    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "44    Convert English to LaTeX\\nEnglish: sum from i ...\n",
       "45    Convert English to LaTeX\\nEnglish: sum from 0 ...\n",
       "46    Convert English to LaTeX\\nEnglish: sum from 0 ...\n",
       "47    Convert English to LaTeX\\nEnglish: sum from 1 ...\n",
       "48    Convert English to LaTeX\\nEnglish: sum from 1 ...\n",
       "49    Convert English to LaTeX\\nEnglish: sum from 1 ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단일 프롬프트 추가\n",
    "CONVERSION_PROMPT = 'Convert English to LaTeX\\n'\n",
    "CONVERSION_TOKEN = 'LaTeX:'\n",
    "\n",
    "# This is our \"training prompt\" that we want GPT2 to recognize and learn\n",
    "training_examples = f'{CONVERSION_PROMPT}English: ' + data['English'] + '\\n' + CONVERSION_TOKEN + ' ' + data['LaTeX'].astype(str)\n",
    "\n",
    "training_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb1b918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Convert English to LaTeX\\nEnglish: integral fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Convert English to LaTeX\\nEnglish: integral fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Convert English to LaTeX\\nEnglish: integral fr...\n",
       "1  Convert English to LaTeX\\nEnglish: integral fr..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_df = pd.DataFrame({'text': training_examples})\n",
    "\n",
    "task_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c98cd94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81094c87-89fc-4bf0-9ccc-4e1d1273ad4b",
   "metadata": {},
   "source": [
    "이 코드는 `task_df`라는 데이터프레임의 'text' 열에 있는 각 문자열의 끝에 토크나이저의 EOS (End of Sequence) 토큰을 추가하는 작업을 수행합니다.\n",
    "\n",
    "코드를 자세히 설명하면 다음과 같습니다:\n",
    "\n",
    "1. `task_df['text']`: `task_df` 데이터프레임의 'text' 열을 선택합니다. 이 열은 텍스트 데이터를 포함하고 있습니다.\n",
    "\n",
    "2. `map(lambda x: f'{x}{tokenizer.eos_token}')`: 'text' 열의 각 문자열에 대해 람다 함수를 적용합니다.\n",
    "   - `lambda x: f'{x}{tokenizer.eos_token}'`: 람다 함수는 입력 문자열 `x`를 받아 해당 문자열의 끝에 `tokenizer.eos_token`을 추가한 새로운 문자열을 반환합니다.\n",
    "   - `f'{x}{tokenizer.eos_token}'`: f-string 포맷팅을 사용하여 입력 문자열 `x`와 `tokenizer.eos_token`을 연결합니다.\n",
    "   - `tokenizer.eos_token`: 토크나이저의 EOS 토큰을 나타냅니다. 이 토큰은 문장이나 시퀀스의 끝을 표시하는 특수 토큰입니다.\n",
    "\n",
    "3. `task_df['text'] = ...`: 람다 함수를 적용한 결과로 생성된 새로운 문자열들로 'text' 열을 업데이트합니다.\n",
    "\n",
    "이 코드의 목적은 텍스트 데이터를 토크나이저로 처리하기 전에 각 문장이나 시퀀스의 끝을 명시적으로 표시하는 것입니다. EOS 토큰을 추가함으로써 토크나이저는 문장의 끝을 인식할 수 있게 됩니다.\n",
    "\n",
    "예를 들어, 다음과 같은 텍스트 데이터가 있다고 가정해보겠습니다:\n",
    "```\n",
    "\"Hello, how are you?\"\n",
    "\"I'm doing fine, thanks for asking.\"\n",
    "```\n",
    "\n",
    "위의 코드를 적용하면 각 문장의 끝에 EOS 토큰이 추가됩니다:\n",
    "```\n",
    "\"Hello, how are you?<eos>\"\n",
    "\"I'm doing fine, thanks for asking.<eos>\"\n",
    "```\n",
    "\n",
    "이렇게 EOS 토큰이 추가된 텍스트 데이터는 토크나이저로 처리될 때 문장의 경계를 명확히 인식할 수 있습니다. 이는 특히 시퀀스 투 시퀀스(Sequence-to-Sequence) 모델이나 언어 모델링 작업에서 중요합니다.\n",
    "\n",
    "EOS 토큰을 추가하는 것은 텍스트 데이터를 토크나이저로 처리하기 전에 일반적으로 수행되는 전처리 과정 중 하나입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7617abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막에 EOS 토큰을 추가하여 모델이 예측을 중단할 시점을 알 수 있도록 합니다.\n",
    "\n",
    "task_df['text'] = task_df['text'].map(lambda x: f'{x}{tokenizer.eos_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76552d13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  Convert English to LaTeX\\nEnglish: integral fr...\n",
      "1  Convert English to LaTeX\\nEnglish: integral fr...\n"
     ]
    }
   ],
   "source": [
    "print(task_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6069b4b-ad1c-4f97-b997-d6fbe6e63f72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Convert English to LaTeX\\nEnglish: integral from a to b of x squared\\nLaTeX: \\\\int_{a}^{b} x^2 \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: integral from negative 1 to 1 of x squared\\nLaTeX: \\\\int_{-1}^{1} x^2 \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: integral from negative 1 to infinity of x cubed\\nLaTeX: \\\\int_{-1}^{\\\\inf} x^3 \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: integral from 0 to infinity of x squared\\nLaTeX: \\\\int_{0}^{\\\\inf} x^2 \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: integral from 0 to infinity of y squared\\nLaTeX: \\\\int_{0}^{\\\\inf} y^2 \\\\,dy<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: integral from 1 to 2 of x over 2\\nLaTeX: \\\\int_{1}^{2} \\\\frac{x}{2} \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals x squared\\nLaTeX: f(x) = x^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: h of x equals x squared\\nLaTeX: h(x) = x^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: g of x equals x squared\\nLaTeX: g(x) = x^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: g of x equals x to the eighth power\\nLaTeX: g(x) = x^8<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals x cubed\\nLaTeX: f(x) = x^3<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals x\\nLaTeX: f(x) = x<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: h of x equals x to the fifth power\\nLaTeX: h(x) = x^5<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: g of x equals integral from 0 to 10 of x cubed\\nLaTeX: g(x) = \\\\int_{0}^{10} x^3 \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals x over n\\nLaTeX: f(x) = \\\\frac{x}{n}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals integral from 1 to 2 of x\\nLaTeX: f(x) = \\\\int_{1}^{2} x \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals integral from 0 to 2 of x\\nLaTeX: f(x) = \\\\int_{0}^{2} x \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals integral from 1 to 2 of x over 2\\nLaTeX: f(x) = \\\\int_{1}^{2} \\\\frac{x}{2} \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals sum from 1 to 5 of x squared\\nLaTeX: f(x) = \\\\sum_{1}^{5} x^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: x squared\\nLaTeX: x^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: x cubed\\nLaTeX: x^3<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: pi squared\\nLaTeX: \\\\pi^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: z squared\\nLaTeX: z^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: z over x squared\\nLaTeX: \\\\frac{z}{x^2}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals x squared\\nLaTeX: f(x) = x^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: 1 over 6\\nLaTeX: \\\\frac{1}{6}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: 2 pi\\nLaTeX: 2 * \\\\pi<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: s cubed\\nLaTeX: s^3<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: s to the sixth power\\nLaTeX: s^6<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: 2 pi r\\nLaTeX: 2 * \\\\pi * r<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: pi over n\\nLaTeX: \\\\frac{\\\\pi}{n}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of n equals pi over n\\nLaTeX: f(n) = \\\\frac{\\\\pi}{n}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: pi times x\\nLaTeX: \\\\pi*x<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: pi to the fourth power\\nLaTeX: \\\\pi^4<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: pi to the fifth power\\nLaTeX: \\\\pi^5<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals x times pi to the fifth power\\nLaTeX: f(x) = x * \\\\pi^5<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: g of x equals x times pi cubed\\nLaTeX: g(x) = x * \\\\pi^3<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: g of x equals pi cubed\\nLaTeX: g(x) = \\\\pi^3<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: 1 over n\\nLaTeX: \\\\frac{1}{n}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: x squared over n\\nLaTeX: \\\\frac{x^2}{n}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: y squared over x^2\\nLaTeX: \\\\frac{y^2}{x^2}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: 1 over 7 to the seventh power\\nLaTeX: (\\\\frac{1}{7})^7<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: 1 over 9 to the seventh power\\nLaTeX: (\\\\frac{1}{9})^7<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals x over 9 to the seventh power\\nLaTeX: (f(x) = \\\\frac{x}{9})^7<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: sum from i to n of X i\\nLaTeX: \\\\sum_{i}^{n} X_i<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: sum from 0 to n of 77 n\\nLaTeX: \\\\sum_{0}^{n} 77 * n<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: sum from 0 to 5 of x\\nLaTeX: \\\\sum_{0}^{5} x<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: sum from 1 to x of x\\nLaTeX: \\\\sum_{1}^{x} x<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: sum from 1 to x of x squared\\nLaTeX: \\\\sum_{1}^{x} x^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: sum from 1 to 10 of pi squared\\nLaTeX: \\\\sum_{1}^{10} \\\\pi^2<|endoftext|><|endoftext|>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_data = Dataset.from_pandas(task_df)  # turn a pandas DataFrame into a Dataset\n",
    "latex_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "074754cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea6f0abe013403095fa1f3915f64e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latex_data = Dataset.from_pandas(task_df)  # turn a pandas DataFrame into a Dataset\n",
    "\n",
    "def preprocess(examples):  \n",
    "    # tokenize our text but don't pad because our collator will pad for us dynamically\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "latex_data = latex_data.map(preprocess, batched=True)\n",
    "\n",
    "latex_data = latex_data.train_test_split(train_size=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16bb726f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Convert English to LaTeX\\nEnglish: 2 pi r\\nLaTeX: 2 * \\\\pi * r<|endoftext|><|endoftext|>',\n",
       " 'input_ids': [3103,\n",
       "  1851,\n",
       "  3594,\n",
       "  284,\n",
       "  4689,\n",
       "  49568,\n",
       "  198,\n",
       "  15823,\n",
       "  25,\n",
       "  362,\n",
       "  31028,\n",
       "  374,\n",
       "  198,\n",
       "  14772,\n",
       "  49568,\n",
       "  25,\n",
       "  362,\n",
       "  1635,\n",
       "  3467,\n",
       "  14415,\n",
       "  1635,\n",
       "  374,\n",
       "  50256,\n",
       "  50256],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f1f30f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동 회귀 언어 모델링을 위한 표준 데이터 콜레이터\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e44dd292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latex_gpt2 = AutoModelForCausalLM.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0712e12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd824d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80924236",
   "metadata": {},
   "source": [
    "# 3. LaTex 변환작업으로 GPT2 파인튜닝하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc2a87-6363-4d0a-95c3-1da1d0c3cb10",
   "metadata": {},
   "source": [
    "이 코드는 Hugging Face의 `transformers` 라이브러리에서 제공하는 `TrainingArguments` 클래스를 사용하여 모델 학습을 위한 하이퍼파라미터와 설정을 지정하는 것입니다.\n",
    "\n",
    "`TrainingArguments`는 모델 학습 과정을 제어하는 다양한 옵션을 제공하며, 이를 통해 학습 과정을 커스터마이즈할 수 있습니다.\n",
    "\n",
    "코드를 자세히 살펴보겠습니다:\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./english_to_latex\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=20,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=5,\n",
    "    log_level='info',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    use_mps_device=True\n",
    ")\n",
    "```\n",
    "\n",
    "- `output_dir=\"./english_to_latex\"`: 모델의 체크포인트와 로그 파일이 저장될 출력 디렉토리를 지정합니다.\n",
    "\n",
    "- `overwrite_output_dir=True`: 출력 디렉토리가 이미 존재하는 경우 해당 디렉토리의 내용을 덮어쓰도록 설정합니다.\n",
    "\n",
    "- `num_train_epochs=5`: 학습 에포크(epoch) 수를 지정합니다. 전체 데이터셋을 5번 반복하여 학습합니다.\n",
    "\n",
    "- `per_device_train_batch_size=1`: 학습 시 각 디바이스(GPU 또는 CPU)에서 사용할 배치 크기를 지정합니다. 여기서는 1로 설정되어 있습니다.\n",
    "\n",
    "- `per_device_eval_batch_size=20`: 평가 시 각 디바이스에서 사용할 배치 크기를 지정합니다. 여기서는 20으로 설정되어 있습니다.\n",
    "\n",
    "- `load_best_model_at_end=True`: 학습이 끝난 후 검증 손실(validation loss)이 가장 낮은 모델을 로드하도록 설정합니다.\n",
    "\n",
    "- `logging_steps=5`: 로깅 간격을 지정합니다. 매 5번째 스텝마다 로그를 출력합니다.\n",
    "\n",
    "- `log_level='info'`: 로그 레벨을 설정합니다. 여기서는 'info' 레벨로 설정되어 있습니다.\n",
    "\n",
    "- `evaluation_strategy='epoch'`: 평가 전략을 설정합니다. 여기서는 'epoch'으로 설정되어 있어 매 에포크마다 평가를 수행합니다.\n",
    "\n",
    "- `save_strategy='epoch'`: 모델 저장 전략을 설정합니다. 여기서는 'epoch'으로 설정되어 있어 매 에포크마다 모델을 저장합니다.\n",
    "\n",
    "- `use_mps_device=True`: MPS(macOS 머신 러닝 가속) 디바이스를 사용하도록 설정합니다. 이는 macOS에서 사용 가능한 GPU 가속 기능입니다.\n",
    "\n",
    "이렇게 설정된 `TrainingArguments`는 `Trainer` 클래스의 `args` 매개변수에 전달되어 학습 과정을 제어하는 데 사용됩니다.\n",
    "\n",
    "```python\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "```\n",
    "\n",
    "위의 코드와 같이 `Trainer` 클래스를 초기화할 때 `training_args`를 전달함으로써 지정된 하이퍼파라미터와 설정에 따라 모델 학습이 수행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d11ea720-1460-4de0-8b92-6f53916f9290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "/Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages/transformers/training_args.py:1951: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of 🤗 Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./english_to_latex\",\n",
    "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
    "    num_train_epochs=5, # number of training epochs\n",
    "    per_device_train_batch_size=1, # batch size for training\n",
    "    per_device_eval_batch_size=20,  # batch size for evaluation\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=5,\n",
    "    log_level='info',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    use_mps_device=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f9931-7271-4ad8-8406-330d18e5fdba",
   "metadata": {},
   "source": [
    "이 코드는 Hugging Face의 `transformers` 라이브러리에서 제공하는 `Trainer` 클래스를 사용하여 모델 학습을 위한 트레이너(Trainer)를 생성하는 것입니다.\n",
    "\n",
    "`Trainer` 클래스는 모델 학습 과정을 관리하고 실행하는 역할을 합니다. 트레이너는 모델, 학습 인자, 데이터셋, 데이터 콜레이터 등을 받아 학습을 수행합니다.\n",
    "\n",
    "코드를 자세히 살펴보겠습니다:\n",
    "\n",
    "```python\n",
    "trainer = Trainer(\n",
    "    model=latex_gpt2,\n",
    "    args=training_args,\n",
    "    train_dataset=latex_data[\"train\"],\n",
    "    eval_dataset=latex_data[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "```\n",
    "\n",
    "- `model=latex_gpt2`: 학습할 모델을 지정합니다. 여기서는 `latex_gpt2`라는 사전 학습된 GPT-2 모델을 사용합니다.\n",
    "\n",
    "- `args=training_args`: 이전에 정의한 `TrainingArguments` 객체를 전달합니다. 이 객체는 학습 과정을 제어하는 다양한 하이퍼파라미터와 설정을 포함하고 있습니다.\n",
    "\n",
    "- `train_dataset=latex_data[\"train\"]`: 학습에 사용할 훈련 데이터셋을 지정합니다. `latex_data`는 데이터셋을 담고 있는 딕셔너리이며, `\"train\"`은 훈련 데이터셋을 나타내는 키입니다.\n",
    "\n",
    "- `eval_dataset=latex_data[\"test\"]`: 평가에 사용할 검증 데이터셋을 지정합니다. `\"test\"`는 검증 데이터셋을 나타내는 키입니다.\n",
    "\n",
    "- `data_collator=data_collator`: 데이터 콜레이터를 지정합니다. 데이터 콜레이터는 데이터셋에서 배치를 생성하고 전처리하는 역할을 합니다. 이전에 정의한 `data_collator`를 사용합니다.\n",
    "\n",
    "이렇게 생성된 `trainer` 객체를 사용하여 모델 학습을 수행할 수 있습니다. `trainer`는 다음과 같은 주요 메서드를 제공합니다:\n",
    "\n",
    "- `train()`: 모델 학습을 시작합니다. 지정된 에포크 수만큼 학습을 반복합니다.\n",
    "- `evaluate()`: 모델을 평가합니다. 검증 데이터셋을 사용하여 모델의 성능을 측정합니다.\n",
    "- `save_model()`: 학습된 모델을 저장합니다.\n",
    "\n",
    "예를 들어, 다음과 같이 `trainer.train()`을 호출하여 모델 학습을 시작할 수 있습니다:\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "학습이 완료되면 `trainer.evaluate()`를 호출하여 모델의 성능을 평가할 수 있습니다:\n",
    "\n",
    "```python\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "```\n",
    "\n",
    "이를 통해 모델 학습 과정을 간편하게 관리하고 실행할 수 있으며, 학습된 모델을 평가하고 저장할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d7582bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=latex_gpt2,\n",
    "    args=training_args,\n",
    "    train_dataset=latex_data[\"train\"],\n",
    "    eval_dataset=latex_data[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b1c0d501-5e6f-49c9-9054-aa5dd16cc3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.351103782653809,\n",
       " 'eval_runtime': 1.6081,\n",
       " 'eval_samples_per_second': 6.219,\n",
       " 'eval_steps_per_second': 0.622}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b78c75d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 40\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 200\n",
      "  Number of trainable parameters = 124,439,808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.083200</td>\n",
       "      <td>7.226024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.534100</td>\n",
       "      <td>8.735909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.613000</td>\n",
       "      <td>9.669665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>9.084322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>9.225646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./english_to_latex/checkpoint-40 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./english_to_latex/checkpoint-40\n",
      "Configuration saved in ./english_to_latex/checkpoint-40/config.json\n",
      "Configuration saved in ./english_to_latex/checkpoint-40/generation_config.json\n",
      "Model weights saved in ./english_to_latex/checkpoint-40/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./english_to_latex/checkpoint-80 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./english_to_latex/checkpoint-80\n",
      "Configuration saved in ./english_to_latex/checkpoint-80/config.json\n",
      "Configuration saved in ./english_to_latex/checkpoint-80/generation_config.json\n",
      "Model weights saved in ./english_to_latex/checkpoint-80/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./english_to_latex/checkpoint-120 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./english_to_latex/checkpoint-120\n",
      "Configuration saved in ./english_to_latex/checkpoint-120/config.json\n",
      "Configuration saved in ./english_to_latex/checkpoint-120/generation_config.json\n",
      "Model weights saved in ./english_to_latex/checkpoint-120/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./english_to_latex/checkpoint-160 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./english_to_latex/checkpoint-160\n",
      "Configuration saved in ./english_to_latex/checkpoint-160/config.json\n",
      "Configuration saved in ./english_to_latex/checkpoint-160/generation_config.json\n",
      "Model weights saved in ./english_to_latex/checkpoint-160/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./english_to_latex/checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./english_to_latex/checkpoint-200\n",
      "Configuration saved in ./english_to_latex/checkpoint-200/config.json\n",
      "Configuration saved in ./english_to_latex/checkpoint-200/generation_config.json\n",
      "Model weights saved in ./english_to_latex/checkpoint-200/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./english_to_latex/checkpoint-40 (score: 7.2260236740112305).\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.911777811050415, metrics={'train_runtime': 34.4433, 'train_samples_per_second': 5.807, 'train_steps_per_second': 5.807, 'total_flos': 3306977280000.0, 'train_loss': 0.911777811050415, 'epoch': 5.0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48742b95-07e3-4f32-a48f-d76e98314314",
   "metadata": {},
   "source": [
    "# 4. LaTex 가이드로 GPT2 파인튜닝하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e3587fc-e7e6-429e-8ba9-80122ad86da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Loading features from cached file ../data/cached_lm_GPT2TokenizerFast_128_latex-guide-cos423.txt [took 0.001 s]\n"
     ]
    }
   ],
   "source": [
    "book_data = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='../data/latex-guide-cos423.txt',  # train on a LaTeX cheat sheet they made\n",
    "    block_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81f13ca4-6c6b-4d85-b529-cfa063fcfedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,  # MLM is Masked Language Modelling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e6c3637-0d88-462a-99fc-b710da2cb1d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/wlkim/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/wlkim/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /Users/wlkim/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_gpt2 = AutoModelForCausalLM.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "573560fa-ce03-484c-bf3d-7ec6b6538766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "/Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages/transformers/training_args.py:1951: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of 🤗 Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./math_book\",\n",
    "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
    "    num_train_epochs=10, # number of training epochs\n",
    "    per_device_train_batch_size=2, # batch size for training\n",
    "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    use_mps_device=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "173901dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=latex_gpt2,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=book_data.examples[:int(len(book_data.examples)*.8)],\n",
    "    eval_dataset=book_data.examples[int(len(book_data.examples)*.8):]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e466aa20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.566359043121338,\n",
       " 'eval_runtime': 0.6286,\n",
       " 'eval_samples_per_second': 19.091,\n",
       " 'eval_steps_per_second': 1.591}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()  # initial loss for the cheat sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "388afa38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 47\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 124,439,808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 01:00, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.639800</td>\n",
       "      <td>3.688287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.980600</td>\n",
       "      <td>3.111025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.121400</td>\n",
       "      <td>3.067104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.766700</td>\n",
       "      <td>3.038630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.866200</td>\n",
       "      <td>3.006435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.343100</td>\n",
       "      <td>3.044762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.464900</td>\n",
       "      <td>3.067927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.325300</td>\n",
       "      <td>3.037357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.297700</td>\n",
       "      <td>3.038291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.320400</td>\n",
       "      <td>3.049669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-24 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-24\n",
      "Configuration saved in ./math_book/checkpoint-24/config.json\n",
      "Configuration saved in ./math_book/checkpoint-24/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-24/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-48 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-48\n",
      "Configuration saved in ./math_book/checkpoint-48/config.json\n",
      "Configuration saved in ./math_book/checkpoint-48/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-48/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-72 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-72\n",
      "Configuration saved in ./math_book/checkpoint-72/config.json\n",
      "Configuration saved in ./math_book/checkpoint-72/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-72/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-96 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-96\n",
      "Configuration saved in ./math_book/checkpoint-96/config.json\n",
      "Configuration saved in ./math_book/checkpoint-96/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-96/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-120 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-120\n",
      "Configuration saved in ./math_book/checkpoint-120/config.json\n",
      "Configuration saved in ./math_book/checkpoint-120/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-120/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-144 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-144\n",
      "Configuration saved in ./math_book/checkpoint-144/config.json\n",
      "Configuration saved in ./math_book/checkpoint-144/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-144/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-168 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-168\n",
      "Configuration saved in ./math_book/checkpoint-168/config.json\n",
      "Configuration saved in ./math_book/checkpoint-168/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-168/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-192 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-192\n",
      "Configuration saved in ./math_book/checkpoint-192/config.json\n",
      "Configuration saved in ./math_book/checkpoint-192/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-192/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-216 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-216\n",
      "Configuration saved in ./math_book/checkpoint-216/config.json\n",
      "Configuration saved in ./math_book/checkpoint-216/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-216/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-240 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-240\n",
      "Configuration saved in ./math_book/checkpoint-240/config.json\n",
      "Configuration saved in ./math_book/checkpoint-240/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-240/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./math_book/checkpoint-120 (score: 3.0064351558685303).\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=1.8412107666333517, metrics={'train_runtime': 61.0493, 'train_samples_per_second': 7.699, 'train_steps_per_second': 3.931, 'total_flos': 30701813760000.0, 'train_loss': 1.8412107666333517, 'epoch': 10.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "270433fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./math_book\n",
      "Configuration saved in ./math_book/config.json\n",
      "Configuration saved in ./math_book/generation_config.json\n",
      "Model weights saved in ./math_book/model.safetensors\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a90f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c38cbe07",
   "metadata": {},
   "source": [
    "# 5. LaTex 가이드 북으로 학습된 GPT2를 LaTex 변환 데이터로 추가 파인튜닝하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c820b1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./math_book/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"./math_book\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file ./math_book/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./math_book.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file ./math_book/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "/Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages/transformers/training_args.py:1951: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of 🤗 Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.351103782653809,\n",
       " 'eval_runtime': 0.1047,\n",
       " 'eval_samples_per_second': 95.541,\n",
       " 'eval_steps_per_second': 9.554}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load up our gpt pre-trained on latex cheat sheets\n",
    "math_latex_gpt2 = AutoModelForCausalLM.from_pretrained('./math_book')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./math_english_to_latex\",\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=5, # number of training epochs\n",
    "    per_device_train_batch_size=1, # batch size for training\n",
    "    per_device_eval_batch_size=20,  # batch size for evaluation\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=5,\n",
    "    log_level='info',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    use_mps_device=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=math_latex_gpt2,\n",
    "    args=training_args,\n",
    "    train_dataset=latex_data[\"train\"],\n",
    "    eval_dataset=latex_data[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.evaluate()  # loss is starting slightly lower than before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67a53cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 40\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 200\n",
      "  Number of trainable parameters = 124,439,808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>7.418637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.546200</td>\n",
       "      <td>8.269737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.823300</td>\n",
       "      <td>8.608335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.304200</td>\n",
       "      <td>8.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.373600</td>\n",
       "      <td>8.110291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./math_english_to_latex/checkpoint-40 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_english_to_latex/checkpoint-40\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-40/config.json\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-40/generation_config.json\n",
      "Model weights saved in ./math_english_to_latex/checkpoint-40/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./math_english_to_latex/checkpoint-80 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_english_to_latex/checkpoint-80\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-80/config.json\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-80/generation_config.json\n",
      "Model weights saved in ./math_english_to_latex/checkpoint-80/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./math_english_to_latex/checkpoint-120 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_english_to_latex/checkpoint-120\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-120/config.json\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-120/generation_config.json\n",
      "Model weights saved in ./math_english_to_latex/checkpoint-120/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./math_english_to_latex/checkpoint-160 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_english_to_latex/checkpoint-160\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-160/config.json\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-160/generation_config.json\n",
      "Model weights saved in ./math_english_to_latex/checkpoint-160/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./math_english_to_latex/checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_english_to_latex/checkpoint-200\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-200/config.json\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-200/generation_config.json\n",
      "Model weights saved in ./math_english_to_latex/checkpoint-200/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./math_english_to_latex/checkpoint-40 (score: 7.418637275695801).\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.9154342466592789, metrics={'train_runtime': 34.2079, 'train_samples_per_second': 5.847, 'train_steps_per_second': 5.847, 'total_flos': 3306977280000.0, 'train_loss': 0.9154342466592789, 'epoch': 5.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "530c78c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./math_english_to_latex\n",
      "Configuration saved in ./math_english_to_latex/config.json\n",
      "Configuration saved in ./math_english_to_latex/generation_config.json\n",
      "Model weights saved in ./math_english_to_latex/model.safetensors\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()  # save this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8201d951-37ca-4748-bbe2-d76dd8b96772",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 6. 모델 성능 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50bae07f-9438-4b14-8d9f-a795fe288f7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/wlkim/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/wlkim/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /Users/wlkim/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "non_finetuned_latex_generator = pipeline('text-generation', model=loaded_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e55d883-5063-499a-a099-36dbfc13f260",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./math_english_to_latex/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"./math_english_to_latex\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file ./math_english_to_latex/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./math_english_to_latex.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file ./math_english_to_latex/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model = AutoModelForCausalLM.from_pretrained('./math_english_to_latex')\n",
    "latex_generator = pipeline('text-generation', model=loaded_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b2cb777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert English to LaTeX\n",
      "English: g of x equals integral from 0 to 1 of x squared\n",
      "LaTeX: g(x) = x^2}^2\n",
      "English: g(x) = x^2}^\n"
     ]
    }
   ],
   "source": [
    "text_sample = 'g of x equals integral from 0 to 1 of x squared'\n",
    "conversion_text_sample = f'{CONVERSION_PROMPT}English: {text_sample}\\n{CONVERSION_TOKEN}'\n",
    "\n",
    "print(latex_generator(\n",
    "    conversion_text_sample, num_beams=2, early_stopping=True, temperature=0.7,\n",
    "    max_new_tokens=24\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46d13e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert English to LaTeX\n",
      "English: r of x is sum from 0 to x of x squared\n",
      "LaTeX: r(x) = x^2\n",
      "LaTeX: r(x) = x^2\n"
     ]
    }
   ],
   "source": [
    "# Another example\n",
    "text_sample = 'r of x is sum from 0 to x of x squared'\n",
    "conversion_text_sample = f'{CONVERSION_PROMPT}English: {text_sample}\\n{CONVERSION_TOKEN}'\n",
    "\n",
    "print(latex_generator(\n",
    "    conversion_text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ac6c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r of x is sum from 0 to x of x squared\n",
      "English: x^2 = x^2\n",
      "LaTeX: x^2 = x^2\n",
      "LaTeX: x^2 = x^2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_generator(\n",
    "    text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06c44d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_generation.TextGenerationPipeline at 0x34011da50>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_finetuned_latex_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab316df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:453: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert English to LaTeX\n",
      "English: f of x is sum from 0 to x of x squared\n",
      "LaTeX: f(x) = \\sum_{0}^{x} x^2 \\,dx ###\n",
      "LCT\n",
      "English: f of x equals integral from 0 to pi of x to the fourth power\n",
      "LaTeX: f(x) = \\int_{0}^{\\pi} x^4 \\,dx ###\n",
      "LCT\n",
      "English: pi to the 8th power\n",
      "LaTeX: pi to the 8th power\n",
      "LCT\n",
      "English: f(x) = \\sum_{\n"
     ]
    }
   ],
   "source": [
    "# try a few shot with standard gpt2\n",
    "few_shot_prompt = CONVERSION_PROMPT+\"\"\"English: f of x is sum from 0 to x of x squared\n",
    "LaTeX: f(x) = \\sum_{0}^{x} x^2 \\,dx \\\n",
    "###\n",
    "LCT\n",
    "English: f of x equals integral from 0 to pi of x to the fourth power\n",
    "LaTeX: f(x) = \\int_{0}^{\\pi} x^4 \\,dx \\\n",
    "###\n",
    "LCT\n",
    "English: pi to the 8th power\n",
    "LaTeX:\"\"\"\n",
    "\n",
    "print(non_finetuned_latex_generator(\n",
    "    few_shot_prompt, num_beams=1, early_stopping=True, temperature=0.1,\n",
    "    max_length=len(tokenizer.encode(few_shot_prompt)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c87ac747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert English to LaTeX\n",
      "English: r of x is sum from 0 to x of x squared\n",
      "LaTeX: r of x is sum from 0 to x of x squared\n",
      "English: r of x is sum\n"
     ]
    }
   ],
   "source": [
    "# Just ask with standard gpt2\n",
    "print(non_finetuned_latex_generator(\n",
    "    conversion_text_sample, num_beams=1, early_stopping=True, temperature=0.1,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87615fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
