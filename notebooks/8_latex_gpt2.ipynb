{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e044664f",
   "metadata": {},
   "source": [
    "## GPT2ë¥¼ ì´ìš©í•œ LaTex ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4138f-7c63-4813-b2d2-501207dabb16",
   "metadata": {},
   "source": [
    "# 1. ëª¨ë¸ ë¡œë”© ë° ë°ì´í„° ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65cd4e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TextDataset, DataCollatorForLanguageModeling, AutoModelForCausalLM, pipeline, \\\n",
    "                         Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b74db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL = 'gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)  # load up a standard gpt2 model\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # set the pad token to avoid a warning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4a8564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>LaTeX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>integral from a to b of x squared</td>\n",
       "      <td>\\int_{a}^{b} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>integral from negative 1 to 1 of x squared</td>\n",
       "      <td>\\int_{-1}^{1} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      English                   LaTeX\n",
       "0           integral from a to b of x squared   \\int_{a}^{b} x^2 \\,dx\n",
       "1  integral from negative 1 to 1 of x squared  \\int_{-1}^{1} x^2 \\,dx"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/english_to_latex.csv')\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a166ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>LaTeX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>integral from a to b of x squared</td>\n",
       "      <td>\\int_{a}^{b} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>integral from negative 1 to 1 of x squared</td>\n",
       "      <td>\\int_{-1}^{1} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>integral from negative 1 to infinity of x cubed</td>\n",
       "      <td>\\int_{-1}^{\\inf} x^3 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>integral from 0 to infinity of x squared</td>\n",
       "      <td>\\int_{0}^{\\inf} x^2 \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>integral from 0 to infinity of y squared</td>\n",
       "      <td>\\int_{0}^{\\inf} y^2 \\,dy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>integral from 1 to 2 of x over 2</td>\n",
       "      <td>\\int_{1}^{2} \\frac{x}{2} \\,dx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>f of x equals x squared</td>\n",
       "      <td>f(x) = x^2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>h of x equals x squared</td>\n",
       "      <td>h(x) = x^2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>g of x equals x squared</td>\n",
       "      <td>g(x) = x^2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>g of x equals x to the eighth power</td>\n",
       "      <td>g(x) = x^8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           English  \\\n",
       "0                integral from a to b of x squared   \n",
       "1       integral from negative 1 to 1 of x squared   \n",
       "2  integral from negative 1 to infinity of x cubed   \n",
       "3         integral from 0 to infinity of x squared   \n",
       "4         integral from 0 to infinity of y squared   \n",
       "5                 integral from 1 to 2 of x over 2   \n",
       "6                          f of x equals x squared   \n",
       "7                          h of x equals x squared   \n",
       "8                          g of x equals x squared   \n",
       "9              g of x equals x to the eighth power   \n",
       "\n",
       "                           LaTeX  \n",
       "0          \\int_{a}^{b} x^2 \\,dx  \n",
       "1         \\int_{-1}^{1} x^2 \\,dx  \n",
       "2      \\int_{-1}^{\\inf} x^3 \\,dx  \n",
       "3       \\int_{0}^{\\inf} x^2 \\,dx  \n",
       "4       \\int_{0}^{\\inf} y^2 \\,dy  \n",
       "5  \\int_{1}^{2} \\frac{x}{2} \\,dx  \n",
       "6                     f(x) = x^2  \n",
       "7                     h(x) = x^2  \n",
       "8                     g(x) = x^2  \n",
       "9                     g(x) = x^8  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdbfb33-615c-4f90-8250-2f621f299f41",
   "metadata": {},
   "source": [
    "# 2. í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db5ebf1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Convert English to LaTeX\\nEnglish: integral fr...\n",
       "1     Convert English to LaTeX\\nEnglish: integral fr...\n",
       "2     Convert English to LaTeX\\nEnglish: integral fr...\n",
       "3     Convert English to LaTeX\\nEnglish: integral fr...\n",
       "4     Convert English to LaTeX\\nEnglish: integral fr...\n",
       "5     Convert English to LaTeX\\nEnglish: integral fr...\n",
       "6     Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "7     Convert English to LaTeX\\nEnglish: h of x equa...\n",
       "8     Convert English to LaTeX\\nEnglish: g of x equa...\n",
       "9     Convert English to LaTeX\\nEnglish: g of x equa...\n",
       "10    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "11    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "12    Convert English to LaTeX\\nEnglish: h of x equa...\n",
       "13    Convert English to LaTeX\\nEnglish: g of x equa...\n",
       "14    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "15    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "16    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "17    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "18    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "19    Convert English to LaTeX\\nEnglish: x squared\\n...\n",
       "20    Convert English to LaTeX\\nEnglish: x cubed\\nLa...\n",
       "21    Convert English to LaTeX\\nEnglish: pi squared\\...\n",
       "22    Convert English to LaTeX\\nEnglish: z squared\\n...\n",
       "23    Convert English to LaTeX\\nEnglish: z over x sq...\n",
       "24    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "25    Convert English to LaTeX\\nEnglish: 1 over 6\\nL...\n",
       "26    Convert English to LaTeX\\nEnglish: 2 pi\\nLaTeX...\n",
       "27    Convert English to LaTeX\\nEnglish: s cubed\\nLa...\n",
       "28    Convert English to LaTeX\\nEnglish: s to the si...\n",
       "29    Convert English to LaTeX\\nEnglish: 2 pi r\\nLaT...\n",
       "30    Convert English to LaTeX\\nEnglish: pi over n\\n...\n",
       "31    Convert English to LaTeX\\nEnglish: f of n equa...\n",
       "32    Convert English to LaTeX\\nEnglish: pi times x\\...\n",
       "33    Convert English to LaTeX\\nEnglish: pi to the f...\n",
       "34    Convert English to LaTeX\\nEnglish: pi to the f...\n",
       "35    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "36    Convert English to LaTeX\\nEnglish: g of x equa...\n",
       "37    Convert English to LaTeX\\nEnglish: g of x equa...\n",
       "38    Convert English to LaTeX\\nEnglish: 1 over n\\nL...\n",
       "39    Convert English to LaTeX\\nEnglish: x squared o...\n",
       "40    Convert English to LaTeX\\nEnglish: y squared o...\n",
       "41    Convert English to LaTeX\\nEnglish: 1 over 7 to...\n",
       "42    Convert English to LaTeX\\nEnglish: 1 over 9 to...\n",
       "43    Convert English to LaTeX\\nEnglish: f of x equa...\n",
       "44    Convert English to LaTeX\\nEnglish: sum from i ...\n",
       "45    Convert English to LaTeX\\nEnglish: sum from 0 ...\n",
       "46    Convert English to LaTeX\\nEnglish: sum from 0 ...\n",
       "47    Convert English to LaTeX\\nEnglish: sum from 1 ...\n",
       "48    Convert English to LaTeX\\nEnglish: sum from 1 ...\n",
       "49    Convert English to LaTeX\\nEnglish: sum from 1 ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ì¶”ê°€\n",
    "CONVERSION_PROMPT = 'Convert English to LaTeX\\n'\n",
    "CONVERSION_TOKEN = 'LaTeX:'\n",
    "\n",
    "# This is our \"training prompt\" that we want GPT2 to recognize and learn\n",
    "training_examples = f'{CONVERSION_PROMPT}English: ' + data['English'] + '\\n' + CONVERSION_TOKEN + ' ' + data['LaTeX'].astype(str)\n",
    "\n",
    "training_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb1b918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Convert English to LaTeX\\nEnglish: integral fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Convert English to LaTeX\\nEnglish: integral fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Convert English to LaTeX\\nEnglish: integral fr...\n",
       "1  Convert English to LaTeX\\nEnglish: integral fr..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_df = pd.DataFrame({'text': training_examples})\n",
    "\n",
    "task_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c98cd94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81094c87-89fc-4bf0-9ccc-4e1d1273ad4b",
   "metadata": {},
   "source": [
    "ì´ ì½”ë“œëŠ” `task_df`ë¼ëŠ” ë°ì´í„°í”„ë ˆì„ì˜ 'text' ì—´ì— ìˆëŠ” ê° ë¬¸ìì—´ì˜ ëì— í† í¬ë‚˜ì´ì €ì˜ EOS (End of Sequence) í† í°ì„ ì¶”ê°€í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì½”ë“œë¥¼ ìì„¸íˆ ì„¤ëª…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. `task_df['text']`: `task_df` ë°ì´í„°í”„ë ˆì„ì˜ 'text' ì—´ì„ ì„ íƒí•©ë‹ˆë‹¤. ì´ ì—´ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "2. `map(lambda x: f'{x}{tokenizer.eos_token}')`: 'text' ì—´ì˜ ê° ë¬¸ìì—´ì— ëŒ€í•´ ëŒë‹¤ í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
    "   - `lambda x: f'{x}{tokenizer.eos_token}'`: ëŒë‹¤ í•¨ìˆ˜ëŠ” ì…ë ¥ ë¬¸ìì—´ `x`ë¥¼ ë°›ì•„ í•´ë‹¹ ë¬¸ìì—´ì˜ ëì— `tokenizer.eos_token`ì„ ì¶”ê°€í•œ ìƒˆë¡œìš´ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "   - `f'{x}{tokenizer.eos_token}'`: f-string í¬ë§·íŒ…ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ ë¬¸ìì—´ `x`ì™€ `tokenizer.eos_token`ì„ ì—°ê²°í•©ë‹ˆë‹¤.\n",
    "   - `tokenizer.eos_token`: í† í¬ë‚˜ì´ì €ì˜ EOS í† í°ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ í† í°ì€ ë¬¸ì¥ì´ë‚˜ ì‹œí€€ìŠ¤ì˜ ëì„ í‘œì‹œí•˜ëŠ” íŠ¹ìˆ˜ í† í°ì…ë‹ˆë‹¤.\n",
    "\n",
    "3. `task_df['text'] = ...`: ëŒë‹¤ í•¨ìˆ˜ë¥¼ ì ìš©í•œ ê²°ê³¼ë¡œ ìƒì„±ëœ ìƒˆë¡œìš´ ë¬¸ìì—´ë“¤ë¡œ 'text' ì—´ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ì½”ë“œì˜ ëª©ì ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì €ë¡œ ì²˜ë¦¬í•˜ê¸° ì „ì— ê° ë¬¸ì¥ì´ë‚˜ ì‹œí€€ìŠ¤ì˜ ëì„ ëª…ì‹œì ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. EOS í† í°ì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ í† í¬ë‚˜ì´ì €ëŠ” ë¬¸ì¥ì˜ ëì„ ì¸ì‹í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "```\n",
    "\"Hello, how are you?\"\n",
    "\"I'm doing fine, thanks for asking.\"\n",
    "```\n",
    "\n",
    "ìœ„ì˜ ì½”ë“œë¥¼ ì ìš©í•˜ë©´ ê° ë¬¸ì¥ì˜ ëì— EOS í† í°ì´ ì¶”ê°€ë©ë‹ˆë‹¤:\n",
    "```\n",
    "\"Hello, how are you?<eos>\"\n",
    "\"I'm doing fine, thanks for asking.<eos>\"\n",
    "```\n",
    "\n",
    "ì´ë ‡ê²Œ EOS í† í°ì´ ì¶”ê°€ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ëŠ” í† í¬ë‚˜ì´ì €ë¡œ ì²˜ë¦¬ë  ë•Œ ë¬¸ì¥ì˜ ê²½ê³„ë¥¼ ëª…í™•íˆ ì¸ì‹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” íŠ¹íˆ ì‹œí€€ìŠ¤ íˆ¬ ì‹œí€€ìŠ¤(Sequence-to-Sequence) ëª¨ë¸ì´ë‚˜ ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—…ì—ì„œ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "EOS í† í°ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì €ë¡œ ì²˜ë¦¬í•˜ê¸° ì „ì— ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜í–‰ë˜ëŠ” ì „ì²˜ë¦¬ ê³¼ì • ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7617abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë§ˆì§€ë§‰ì— EOS í† í°ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì´ ì˜ˆì¸¡ì„ ì¤‘ë‹¨í•  ì‹œì ì„ ì•Œ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
    "\n",
    "task_df['text'] = task_df['text'].map(lambda x: f'{x}{tokenizer.eos_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76552d13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  Convert English to LaTeX\\nEnglish: integral fr...\n",
      "1  Convert English to LaTeX\\nEnglish: integral fr...\n"
     ]
    }
   ],
   "source": [
    "print(task_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6069b4b-ad1c-4f97-b997-d6fbe6e63f72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Convert English to LaTeX\\nEnglish: integral from a to b of x squared\\nLaTeX: \\\\int_{a}^{b} x^2 \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: integral from negative 1 to 1 of x squared\\nLaTeX: \\\\int_{-1}^{1} x^2 \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: integral from negative 1 to infinity of x cubed\\nLaTeX: \\\\int_{-1}^{\\\\inf} x^3 \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: integral from 0 to infinity of x squared\\nLaTeX: \\\\int_{0}^{\\\\inf} x^2 \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: integral from 0 to infinity of y squared\\nLaTeX: \\\\int_{0}^{\\\\inf} y^2 \\\\,dy<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: integral from 1 to 2 of x over 2\\nLaTeX: \\\\int_{1}^{2} \\\\frac{x}{2} \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals x squared\\nLaTeX: f(x) = x^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: h of x equals x squared\\nLaTeX: h(x) = x^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: g of x equals x squared\\nLaTeX: g(x) = x^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: g of x equals x to the eighth power\\nLaTeX: g(x) = x^8<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals x cubed\\nLaTeX: f(x) = x^3<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals x\\nLaTeX: f(x) = x<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: h of x equals x to the fifth power\\nLaTeX: h(x) = x^5<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: g of x equals integral from 0 to 10 of x cubed\\nLaTeX: g(x) = \\\\int_{0}^{10} x^3 \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals x over n\\nLaTeX: f(x) = \\\\frac{x}{n}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals integral from 1 to 2 of x\\nLaTeX: f(x) = \\\\int_{1}^{2} x \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals integral from 0 to 2 of x\\nLaTeX: f(x) = \\\\int_{0}^{2} x \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals integral from 1 to 2 of x over 2\\nLaTeX: f(x) = \\\\int_{1}^{2} \\\\frac{x}{2} \\\\,dx<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals sum from 1 to 5 of x squared\\nLaTeX: f(x) = \\\\sum_{1}^{5} x^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: x squared\\nLaTeX: x^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: x cubed\\nLaTeX: x^3<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: pi squared\\nLaTeX: \\\\pi^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: z squared\\nLaTeX: z^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: z over x squared\\nLaTeX: \\\\frac{z}{x^2}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals x squared\\nLaTeX: f(x) = x^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: 1 over 6\\nLaTeX: \\\\frac{1}{6}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: 2 pi\\nLaTeX: 2 * \\\\pi<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: s cubed\\nLaTeX: s^3<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: s to the sixth power\\nLaTeX: s^6<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: 2 pi r\\nLaTeX: 2 * \\\\pi * r<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: pi over n\\nLaTeX: \\\\frac{\\\\pi}{n}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of n equals pi over n\\nLaTeX: f(n) = \\\\frac{\\\\pi}{n}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: pi times x\\nLaTeX: \\\\pi*x<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: pi to the fourth power\\nLaTeX: \\\\pi^4<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: pi to the fifth power\\nLaTeX: \\\\pi^5<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals x times pi to the fifth power\\nLaTeX: f(x) = x * \\\\pi^5<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: g of x equals x times pi cubed\\nLaTeX: g(x) = x * \\\\pi^3<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: g of x equals pi cubed\\nLaTeX: g(x) = \\\\pi^3<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: 1 over n\\nLaTeX: \\\\frac{1}{n}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: x squared over n\\nLaTeX: \\\\frac{x^2}{n}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: y squared over x^2\\nLaTeX: \\\\frac{y^2}{x^2}<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: 1 over 7 to the seventh power\\nLaTeX: (\\\\frac{1}{7})^7<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: 1 over 9 to the seventh power\\nLaTeX: (\\\\frac{1}{9})^7<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: f of x equals x over 9 to the seventh power\\nLaTeX: (f(x) = \\\\frac{x}{9})^7<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: sum from i to n of X i\\nLaTeX: \\\\sum_{i}^{n} X_i<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: sum from 0 to n of 77 n\\nLaTeX: \\\\sum_{0}^{n} 77 * n<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: sum from 0 to 5 of x\\nLaTeX: \\\\sum_{0}^{5} x<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: sum from 1 to x of x\\nLaTeX: \\\\sum_{1}^{x} x<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: sum from 1 to x of x squared\\nLaTeX: \\\\sum_{1}^{x} x^2<|endoftext|><|endoftext|>',\n",
       " 'Convert English to LaTeX\\nEnglish: sum from 1 to 10 of pi squared\\nLaTeX: \\\\sum_{1}^{10} \\\\pi^2<|endoftext|><|endoftext|>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_data = Dataset.from_pandas(task_df)  # turn a pandas DataFrame into a Dataset\n",
    "latex_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "074754cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea6f0abe013403095fa1f3915f64e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latex_data = Dataset.from_pandas(task_df)  # turn a pandas DataFrame into a Dataset\n",
    "\n",
    "def preprocess(examples):  \n",
    "    # tokenize our text but don't pad because our collator will pad for us dynamically\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "latex_data = latex_data.map(preprocess, batched=True)\n",
    "\n",
    "latex_data = latex_data.train_test_split(train_size=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16bb726f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Convert English to LaTeX\\nEnglish: 2 pi r\\nLaTeX: 2 * \\\\pi * r<|endoftext|><|endoftext|>',\n",
       " 'input_ids': [3103,\n",
       "  1851,\n",
       "  3594,\n",
       "  284,\n",
       "  4689,\n",
       "  49568,\n",
       "  198,\n",
       "  15823,\n",
       "  25,\n",
       "  362,\n",
       "  31028,\n",
       "  374,\n",
       "  198,\n",
       "  14772,\n",
       "  49568,\n",
       "  25,\n",
       "  362,\n",
       "  1635,\n",
       "  3467,\n",
       "  14415,\n",
       "  1635,\n",
       "  374,\n",
       "  50256,\n",
       "  50256],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f1f30f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìë™ íšŒê·€ ì–¸ì–´ ëª¨ë¸ë§ì„ ìœ„í•œ í‘œì¤€ ë°ì´í„° ì½œë ˆì´í„°\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e44dd292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latex_gpt2 = AutoModelForCausalLM.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0712e12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd824d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80924236",
   "metadata": {},
   "source": [
    "# 3. LaTex ë³€í™˜ì‘ì—…ìœ¼ë¡œ GPT2 íŒŒì¸íŠœë‹í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc2a87-6363-4d0a-95c3-1da1d0c3cb10",
   "metadata": {},
   "source": [
    "ì´ ì½”ë“œëŠ” Hugging Faceì˜ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” `TrainingArguments` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„¤ì •ì„ ì§€ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "`TrainingArguments`ëŠ” ëª¨ë¸ í•™ìŠµ ê³¼ì •ì„ ì œì–´í•˜ëŠ” ë‹¤ì–‘í•œ ì˜µì…˜ì„ ì œê³µí•˜ë©°, ì´ë¥¼ í†µí•´ í•™ìŠµ ê³¼ì •ì„ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì½”ë“œë¥¼ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./english_to_latex\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=20,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=5,\n",
    "    log_level='info',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    use_mps_device=True\n",
    ")\n",
    "```\n",
    "\n",
    "- `output_dir=\"./english_to_latex\"`: ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ì™€ ë¡œê·¸ íŒŒì¼ì´ ì €ì¥ë  ì¶œë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `overwrite_output_dir=True`: ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° í•´ë‹¹ ë””ë ‰í† ë¦¬ì˜ ë‚´ìš©ì„ ë®ì–´ì“°ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `num_train_epochs=5`: í•™ìŠµ ì—í¬í¬(epoch) ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ì…‹ì„ 5ë²ˆ ë°˜ë³µí•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "- `per_device_train_batch_size=1`: í•™ìŠµ ì‹œ ê° ë””ë°”ì´ìŠ¤(GPU ë˜ëŠ” CPU)ì—ì„œ ì‚¬ìš©í•  ë°°ì¹˜ í¬ê¸°ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 1ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- `per_device_eval_batch_size=20`: í‰ê°€ ì‹œ ê° ë””ë°”ì´ìŠ¤ì—ì„œ ì‚¬ìš©í•  ë°°ì¹˜ í¬ê¸°ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 20ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- `load_best_model_at_end=True`: í•™ìŠµì´ ëë‚œ í›„ ê²€ì¦ ì†ì‹¤(validation loss)ì´ ê°€ì¥ ë‚®ì€ ëª¨ë¸ì„ ë¡œë“œí•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `logging_steps=5`: ë¡œê¹… ê°„ê²©ì„ ì§€ì •í•©ë‹ˆë‹¤. ë§¤ 5ë²ˆì§¸ ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `log_level='info'`: ë¡œê·¸ ë ˆë²¨ì„ ì„¤ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 'info' ë ˆë²¨ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- `evaluation_strategy='epoch'`: í‰ê°€ ì „ëµì„ ì„¤ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 'epoch'ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ ë§¤ ì—í¬í¬ë§ˆë‹¤ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `save_strategy='epoch'`: ëª¨ë¸ ì €ì¥ ì „ëµì„ ì„¤ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 'epoch'ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ ë§¤ ì—í¬í¬ë§ˆë‹¤ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `use_mps_device=True`: MPS(macOS ë¨¸ì‹  ëŸ¬ë‹ ê°€ì†) ë””ë°”ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤. ì´ëŠ” macOSì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°€ì† ê¸°ëŠ¥ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ë ‡ê²Œ ì„¤ì •ëœ `TrainingArguments`ëŠ” `Trainer` í´ë˜ìŠ¤ì˜ `args` ë§¤ê°œë³€ìˆ˜ì— ì „ë‹¬ë˜ì–´ í•™ìŠµ ê³¼ì •ì„ ì œì–´í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "```\n",
    "\n",
    "ìœ„ì˜ ì½”ë“œì™€ ê°™ì´ `Trainer` í´ë˜ìŠ¤ë¥¼ ì´ˆê¸°í™”í•  ë•Œ `training_args`ë¥¼ ì „ë‹¬í•¨ìœ¼ë¡œì¨ ì§€ì •ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„¤ì •ì— ë”°ë¼ ëª¨ë¸ í•™ìŠµì´ ìˆ˜í–‰ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d11ea720-1460-4de0-8b92-6f53916f9290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "/Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages/transformers/training_args.py:1951: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ğŸ¤— Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./english_to_latex\",\n",
    "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
    "    num_train_epochs=5, # number of training epochs\n",
    "    per_device_train_batch_size=1, # batch size for training\n",
    "    per_device_eval_batch_size=20,  # batch size for evaluation\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=5,\n",
    "    log_level='info',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    use_mps_device=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f9931-7271-4ad8-8406-330d18e5fdba",
   "metadata": {},
   "source": [
    "ì´ ì½”ë“œëŠ” Hugging Faceì˜ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” `Trainer` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ íŠ¸ë ˆì´ë„ˆ(Trainer)ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "`Trainer` í´ë˜ìŠ¤ëŠ” ëª¨ë¸ í•™ìŠµ ê³¼ì •ì„ ê´€ë¦¬í•˜ê³  ì‹¤í–‰í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. íŠ¸ë ˆì´ë„ˆëŠ” ëª¨ë¸, í•™ìŠµ ì¸ì, ë°ì´í„°ì…‹, ë°ì´í„° ì½œë ˆì´í„° ë“±ì„ ë°›ì•„ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì½”ë“œë¥¼ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "\n",
    "```python\n",
    "trainer = Trainer(\n",
    "    model=latex_gpt2,\n",
    "    args=training_args,\n",
    "    train_dataset=latex_data[\"train\"],\n",
    "    eval_dataset=latex_data[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "```\n",
    "\n",
    "- `model=latex_gpt2`: í•™ìŠµí•  ëª¨ë¸ì„ ì§€ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” `latex_gpt2`ë¼ëŠ” ì‚¬ì „ í•™ìŠµëœ GPT-2 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `args=training_args`: ì´ì „ì— ì •ì˜í•œ `TrainingArguments` ê°ì²´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤. ì´ ê°ì²´ëŠ” í•™ìŠµ ê³¼ì •ì„ ì œì–´í•˜ëŠ” ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„¤ì •ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- `train_dataset=latex_data[\"train\"]`: í•™ìŠµì— ì‚¬ìš©í•  í›ˆë ¨ ë°ì´í„°ì…‹ì„ ì§€ì •í•©ë‹ˆë‹¤. `latex_data`ëŠ” ë°ì´í„°ì…‹ì„ ë‹´ê³  ìˆëŠ” ë”•ì…”ë„ˆë¦¬ì´ë©°, `\"train\"`ì€ í›ˆë ¨ ë°ì´í„°ì…‹ì„ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì…ë‹ˆë‹¤.\n",
    "\n",
    "- `eval_dataset=latex_data[\"test\"]`: í‰ê°€ì— ì‚¬ìš©í•  ê²€ì¦ ë°ì´í„°ì…‹ì„ ì§€ì •í•©ë‹ˆë‹¤. `\"test\"`ëŠ” ê²€ì¦ ë°ì´í„°ì…‹ì„ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì…ë‹ˆë‹¤.\n",
    "\n",
    "- `data_collator=data_collator`: ë°ì´í„° ì½œë ˆì´í„°ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ë°ì´í„° ì½œë ˆì´í„°ëŠ” ë°ì´í„°ì…‹ì—ì„œ ë°°ì¹˜ë¥¼ ìƒì„±í•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ì „ì— ì •ì˜í•œ `data_collator`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ë ‡ê²Œ ìƒì„±ëœ `trainer` ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `trainer`ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ë©”ì„œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤:\n",
    "\n",
    "- `train()`: ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤. ì§€ì •ëœ ì—í¬í¬ ìˆ˜ë§Œí¼ í•™ìŠµì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "- `evaluate()`: ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤. ê²€ì¦ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
    "- `save_model()`: í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì´ `trainer.train()`ì„ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "í•™ìŠµì´ ì™„ë£Œë˜ë©´ `trainer.evaluate()`ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "```python\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "```\n",
    "\n",
    "ì´ë¥¼ í†µí•´ ëª¨ë¸ í•™ìŠµ ê³¼ì •ì„ ê°„í¸í•˜ê²Œ ê´€ë¦¬í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆìœ¼ë©°, í•™ìŠµëœ ëª¨ë¸ì„ í‰ê°€í•˜ê³  ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d7582bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=latex_gpt2,\n",
    "    args=training_args,\n",
    "    train_dataset=latex_data[\"train\"],\n",
    "    eval_dataset=latex_data[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b1c0d501-5e6f-49c9-9054-aa5dd16cc3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.351103782653809,\n",
       " 'eval_runtime': 1.6081,\n",
       " 'eval_samples_per_second': 6.219,\n",
       " 'eval_steps_per_second': 0.622}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b78c75d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 40\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 200\n",
      "  Number of trainable parameters = 124,439,808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.083200</td>\n",
       "      <td>7.226024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.534100</td>\n",
       "      <td>8.735909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.613000</td>\n",
       "      <td>9.669665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>9.084322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>9.225646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./english_to_latex/checkpoint-40 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./english_to_latex/checkpoint-40\n",
      "Configuration saved in ./english_to_latex/checkpoint-40/config.json\n",
      "Configuration saved in ./english_to_latex/checkpoint-40/generation_config.json\n",
      "Model weights saved in ./english_to_latex/checkpoint-40/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./english_to_latex/checkpoint-80 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./english_to_latex/checkpoint-80\n",
      "Configuration saved in ./english_to_latex/checkpoint-80/config.json\n",
      "Configuration saved in ./english_to_latex/checkpoint-80/generation_config.json\n",
      "Model weights saved in ./english_to_latex/checkpoint-80/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./english_to_latex/checkpoint-120 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./english_to_latex/checkpoint-120\n",
      "Configuration saved in ./english_to_latex/checkpoint-120/config.json\n",
      "Configuration saved in ./english_to_latex/checkpoint-120/generation_config.json\n",
      "Model weights saved in ./english_to_latex/checkpoint-120/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./english_to_latex/checkpoint-160 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./english_to_latex/checkpoint-160\n",
      "Configuration saved in ./english_to_latex/checkpoint-160/config.json\n",
      "Configuration saved in ./english_to_latex/checkpoint-160/generation_config.json\n",
      "Model weights saved in ./english_to_latex/checkpoint-160/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./english_to_latex/checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./english_to_latex/checkpoint-200\n",
      "Configuration saved in ./english_to_latex/checkpoint-200/config.json\n",
      "Configuration saved in ./english_to_latex/checkpoint-200/generation_config.json\n",
      "Model weights saved in ./english_to_latex/checkpoint-200/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./english_to_latex/checkpoint-40 (score: 7.2260236740112305).\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.911777811050415, metrics={'train_runtime': 34.4433, 'train_samples_per_second': 5.807, 'train_steps_per_second': 5.807, 'total_flos': 3306977280000.0, 'train_loss': 0.911777811050415, 'epoch': 5.0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48742b95-07e3-4f32-a48f-d76e98314314",
   "metadata": {},
   "source": [
    "# 4. LaTex ê°€ì´ë“œë¡œ GPT2 íŒŒì¸íŠœë‹í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e3587fc-e7e6-429e-8ba9-80122ad86da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Loading features from cached file ../data/cached_lm_GPT2TokenizerFast_128_latex-guide-cos423.txt [took 0.001 s]\n"
     ]
    }
   ],
   "source": [
    "book_data = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='../data/latex-guide-cos423.txt',  # train on a LaTeX cheat sheet they made\n",
    "    block_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81f13ca4-6c6b-4d85-b529-cfa063fcfedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,  # MLM is Masked Language Modelling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e6c3637-0d88-462a-99fc-b710da2cb1d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/wlkim/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/wlkim/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /Users/wlkim/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_gpt2 = AutoModelForCausalLM.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "573560fa-ce03-484c-bf3d-7ec6b6538766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "/Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages/transformers/training_args.py:1951: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ğŸ¤— Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./math_book\",\n",
    "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
    "    num_train_epochs=10, # number of training epochs\n",
    "    per_device_train_batch_size=2, # batch size for training\n",
    "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    use_mps_device=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "173901dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=latex_gpt2,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=book_data.examples[:int(len(book_data.examples)*.8)],\n",
    "    eval_dataset=book_data.examples[int(len(book_data.examples)*.8):]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e466aa20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.566359043121338,\n",
       " 'eval_runtime': 0.6286,\n",
       " 'eval_samples_per_second': 19.091,\n",
       " 'eval_steps_per_second': 1.591}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()  # initial loss for the cheat sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "388afa38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 47\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 240\n",
      "  Number of trainable parameters = 124,439,808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 01:00, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.639800</td>\n",
       "      <td>3.688287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.980600</td>\n",
       "      <td>3.111025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.121400</td>\n",
       "      <td>3.067104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.766700</td>\n",
       "      <td>3.038630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.866200</td>\n",
       "      <td>3.006435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.343100</td>\n",
       "      <td>3.044762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.464900</td>\n",
       "      <td>3.067927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.325300</td>\n",
       "      <td>3.037357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.297700</td>\n",
       "      <td>3.038291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.320400</td>\n",
       "      <td>3.049669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-24 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-24\n",
      "Configuration saved in ./math_book/checkpoint-24/config.json\n",
      "Configuration saved in ./math_book/checkpoint-24/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-24/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-48 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-48\n",
      "Configuration saved in ./math_book/checkpoint-48/config.json\n",
      "Configuration saved in ./math_book/checkpoint-48/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-48/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-72 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-72\n",
      "Configuration saved in ./math_book/checkpoint-72/config.json\n",
      "Configuration saved in ./math_book/checkpoint-72/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-72/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-96 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-96\n",
      "Configuration saved in ./math_book/checkpoint-96/config.json\n",
      "Configuration saved in ./math_book/checkpoint-96/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-96/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-120 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-120\n",
      "Configuration saved in ./math_book/checkpoint-120/config.json\n",
      "Configuration saved in ./math_book/checkpoint-120/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-120/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-144 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-144\n",
      "Configuration saved in ./math_book/checkpoint-144/config.json\n",
      "Configuration saved in ./math_book/checkpoint-144/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-144/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-168 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-168\n",
      "Configuration saved in ./math_book/checkpoint-168/config.json\n",
      "Configuration saved in ./math_book/checkpoint-168/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-168/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-192 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-192\n",
      "Configuration saved in ./math_book/checkpoint-192/config.json\n",
      "Configuration saved in ./math_book/checkpoint-192/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-192/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-216 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-216\n",
      "Configuration saved in ./math_book/checkpoint-216/config.json\n",
      "Configuration saved in ./math_book/checkpoint-216/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-216/model.safetensors\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12\n",
      "  Batch size = 32\n",
      "Checkpoint destination directory ./math_book/checkpoint-240 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_book/checkpoint-240\n",
      "Configuration saved in ./math_book/checkpoint-240/config.json\n",
      "Configuration saved in ./math_book/checkpoint-240/generation_config.json\n",
      "Model weights saved in ./math_book/checkpoint-240/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./math_book/checkpoint-120 (score: 3.0064351558685303).\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=1.8412107666333517, metrics={'train_runtime': 61.0493, 'train_samples_per_second': 7.699, 'train_steps_per_second': 3.931, 'total_flos': 30701813760000.0, 'train_loss': 1.8412107666333517, 'epoch': 10.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "270433fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./math_book\n",
      "Configuration saved in ./math_book/config.json\n",
      "Configuration saved in ./math_book/generation_config.json\n",
      "Model weights saved in ./math_book/model.safetensors\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a90f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c38cbe07",
   "metadata": {},
   "source": [
    "# 5. LaTex ê°€ì´ë“œ ë¶ìœ¼ë¡œ í•™ìŠµëœ GPT2ë¥¼ LaTex ë³€í™˜ ë°ì´í„°ë¡œ ì¶”ê°€ íŒŒì¸íŠœë‹í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c820b1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./math_book/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"./math_book\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file ./math_book/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./math_book.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file ./math_book/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "/Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages/transformers/training_args.py:1951: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ğŸ¤— Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.351103782653809,\n",
       " 'eval_runtime': 0.1047,\n",
       " 'eval_samples_per_second': 95.541,\n",
       " 'eval_steps_per_second': 9.554}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load up our gpt pre-trained on latex cheat sheets\n",
    "math_latex_gpt2 = AutoModelForCausalLM.from_pretrained('./math_book')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./math_english_to_latex\",\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=5, # number of training epochs\n",
    "    per_device_train_batch_size=1, # batch size for training\n",
    "    per_device_eval_batch_size=20,  # batch size for evaluation\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=5,\n",
    "    log_level='info',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    use_mps_device=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=math_latex_gpt2,\n",
    "    args=training_args,\n",
    "    train_dataset=latex_data[\"train\"],\n",
    "    eval_dataset=latex_data[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.evaluate()  # loss is starting slightly lower than before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67a53cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 40\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 200\n",
      "  Number of trainable parameters = 124,439,808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>7.418637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.546200</td>\n",
       "      <td>8.269737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.823300</td>\n",
       "      <td>8.608335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.304200</td>\n",
       "      <td>8.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.373600</td>\n",
       "      <td>8.110291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./math_english_to_latex/checkpoint-40 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_english_to_latex/checkpoint-40\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-40/config.json\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-40/generation_config.json\n",
      "Model weights saved in ./math_english_to_latex/checkpoint-40/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./math_english_to_latex/checkpoint-80 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_english_to_latex/checkpoint-80\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-80/config.json\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-80/generation_config.json\n",
      "Model weights saved in ./math_english_to_latex/checkpoint-80/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./math_english_to_latex/checkpoint-120 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_english_to_latex/checkpoint-120\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-120/config.json\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-120/generation_config.json\n",
      "Model weights saved in ./math_english_to_latex/checkpoint-120/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./math_english_to_latex/checkpoint-160 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_english_to_latex/checkpoint-160\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-160/config.json\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-160/generation_config.json\n",
      "Model weights saved in ./math_english_to_latex/checkpoint-160/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 20\n",
      "Checkpoint destination directory ./math_english_to_latex/checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./math_english_to_latex/checkpoint-200\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-200/config.json\n",
      "Configuration saved in ./math_english_to_latex/checkpoint-200/generation_config.json\n",
      "Model weights saved in ./math_english_to_latex/checkpoint-200/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./math_english_to_latex/checkpoint-40 (score: 7.418637275695801).\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.9154342466592789, metrics={'train_runtime': 34.2079, 'train_samples_per_second': 5.847, 'train_steps_per_second': 5.847, 'total_flos': 3306977280000.0, 'train_loss': 0.9154342466592789, 'epoch': 5.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "530c78c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./math_english_to_latex\n",
      "Configuration saved in ./math_english_to_latex/config.json\n",
      "Configuration saved in ./math_english_to_latex/generation_config.json\n",
      "Model weights saved in ./math_english_to_latex/model.safetensors\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()  # save this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8201d951-37ca-4748-bbe2-d76dd8b96772",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 6. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50bae07f-9438-4b14-8d9f-a795fe288f7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/wlkim/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/wlkim/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /Users/wlkim/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "non_finetuned_latex_generator = pipeline('text-generation', model=loaded_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e55d883-5063-499a-a099-36dbfc13f260",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./math_english_to_latex/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"./math_english_to_latex\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file ./math_english_to_latex/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./math_english_to_latex.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file ./math_english_to_latex/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model = AutoModelForCausalLM.from_pretrained('./math_english_to_latex')\n",
    "latex_generator = pipeline('text-generation', model=loaded_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b2cb777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert English to LaTeX\n",
      "English: g of x equals integral from 0 to 1 of x squared\n",
      "LaTeX: g(x) = x^2}^2\n",
      "English: g(x) = x^2}^\n"
     ]
    }
   ],
   "source": [
    "text_sample = 'g of x equals integral from 0 to 1 of x squared'\n",
    "conversion_text_sample = f'{CONVERSION_PROMPT}English: {text_sample}\\n{CONVERSION_TOKEN}'\n",
    "\n",
    "print(latex_generator(\n",
    "    conversion_text_sample, num_beams=2, early_stopping=True, temperature=0.7,\n",
    "    max_new_tokens=24\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46d13e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert English to LaTeX\n",
      "English: r of x is sum from 0 to x of x squared\n",
      "LaTeX: r(x) = x^2\n",
      "LaTeX: r(x) = x^2\n"
     ]
    }
   ],
   "source": [
    "# Another example\n",
    "text_sample = 'r of x is sum from 0 to x of x squared'\n",
    "conversion_text_sample = f'{CONVERSION_PROMPT}English: {text_sample}\\n{CONVERSION_TOKEN}'\n",
    "\n",
    "print(latex_generator(\n",
    "    conversion_text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ac6c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r of x is sum from 0 to x of x squared\n",
      "English: x^2 = x^2\n",
      "LaTeX: x^2 = x^2\n",
      "LaTeX: x^2 = x^2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_generator(\n",
    "    text_sample, num_beams=5, early_stopping=True, temperature=0.7,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06c44d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_generation.TextGenerationPipeline at 0x34011da50>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_finetuned_latex_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab316df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wlkim/anaconda3/envs/quick-start-guide-to-llms/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:453: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert English to LaTeX\n",
      "English: f of x is sum from 0 to x of x squared\n",
      "LaTeX: f(x) = \\sum_{0}^{x} x^2 \\,dx ###\n",
      "LCT\n",
      "English: f of x equals integral from 0 to pi of x to the fourth power\n",
      "LaTeX: f(x) = \\int_{0}^{\\pi} x^4 \\,dx ###\n",
      "LCT\n",
      "English: pi to the 8th power\n",
      "LaTeX: pi to the 8th power\n",
      "LCT\n",
      "English: f(x) = \\sum_{\n"
     ]
    }
   ],
   "source": [
    "# try a few shot with standard gpt2\n",
    "few_shot_prompt = CONVERSION_PROMPT+\"\"\"English: f of x is sum from 0 to x of x squared\n",
    "LaTeX: f(x) = \\sum_{0}^{x} x^2 \\,dx \\\n",
    "###\n",
    "LCT\n",
    "English: f of x equals integral from 0 to pi of x to the fourth power\n",
    "LaTeX: f(x) = \\int_{0}^{\\pi} x^4 \\,dx \\\n",
    "###\n",
    "LCT\n",
    "English: pi to the 8th power\n",
    "LaTeX:\"\"\"\n",
    "\n",
    "print(non_finetuned_latex_generator(\n",
    "    few_shot_prompt, num_beams=1, early_stopping=True, temperature=0.1,\n",
    "    max_length=len(tokenizer.encode(few_shot_prompt)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c87ac747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert English to LaTeX\n",
      "English: r of x is sum from 0 to x of x squared\n",
      "LaTeX: r of x is sum from 0 to x of x squared\n",
      "English: r of x is sum\n"
     ]
    }
   ],
   "source": [
    "# Just ask with standard gpt2\n",
    "print(non_finetuned_latex_generator(\n",
    "    conversion_text_sample, num_beams=1, early_stopping=True, temperature=0.1,\n",
    "    max_length=len(tokenizer.encode(conversion_text_sample)) + 20\n",
    ")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87615fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
